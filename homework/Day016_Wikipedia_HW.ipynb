{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia爬蟲練習\n",
    "## 範例：練習是從Wikipedia中爬取文章。先定義一個搜尋的關鍵字，擷取該關鍵字詞的文章。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 先定義一個我們想搜尋的字詞，並將它轉換成UTF-8編碼後的URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "網路爬蟲: %E7%B6%B2%E8%B7%AF%E7%88%AC%E8%9F%B2\n",
      "/wiki/%E7%B6%B2%E8%B7%AF%E7%88%AC%E8%9F%B2\n"
     ]
    }
   ],
   "source": [
    "input_keyword = \"網路爬蟲\"  # 這裡可以自己定義有興趣的關鍵字\n",
    "\n",
    "utf8_url = repr(input_keyword.encode('UTF-8')).upper()  # 編碼成UTF-8並轉成大寫字元\n",
    "utf8_url = utf8_url.replace(\"\\\\X\", \"%\")                 # 用 '%' 取代 '\\X' \n",
    "print(\"%s: %s\" % (input_keyword, utf8_url[2:-1:1]))     # 擷取中間的編碼結果\n",
    "\n",
    "# 組成Wiki關鍵字搜尋的網址格式\n",
    "root_keyword_link = '/wiki/' + utf8_url[2:-1:1]\n",
    "print(root_keyword_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 範例1：送出關鍵字請求後，爬取該關鍵字的文章內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "網路爬蟲（英語：web crawler），也叫網路蜘蛛（spider），是一種用來自動瀏覽全球資訊網的網路機器人。其目的一般為編纂網路索引（英語：Web indexing）。\n",
      "\n",
      "網路搜尋引擎等站點通過爬蟲軟體更新自身的網站內容（英語：Web content）或其對其他網站的索引。網路爬蟲可以將自己所存取的頁面儲存下來，以便搜尋引擎事後生成索引（英語：Index (search engine)）供用戶搜尋。\n",
      "\n",
      "爬蟲存取網站的過程會消耗目標系統資源。不少網路系統並不默許爬蟲工作。因此在存取大量頁面時，爬蟲需要考慮到規劃、負載，還需要講「禮貌」。 不願意被爬蟲存取、被爬蟲主人知曉的公開站點可以使用robots.txt檔案之類的方法避免存取。這個檔案可以要求機器人（英語：Software agent）只對網站的一部分進行索引，或完全不作處理。\n",
      "\n",
      "網際網路上的頁面極多，即使是最大的爬蟲系統也無法做出完整的索引。因此在公元2000年之前的全球資訊網出現初期，搜尋引擎經常找不到多少相關結果。現在的搜尋引擎在這方面已經進步很多，能夠即刻給出高品質結果。\n",
      "\n",
      "爬蟲還可以驗證超連結和HTML代碼，用於網路抓取（英語：Web scraping）（參見資料驅動編程（英語：Data-driven programming））。\n",
      "\n",
      "網路爬蟲也可稱作網路蜘蛛[1]、螞蟻、自動索引程式（automatic indexer）[2] ，或（在FOAF（英語：FOAF (software)）軟體中）稱為網路疾走（web scutter）。[3]\n",
      "\n",
      "網路爬蟲始於一張被稱作種子的統一資源位址（URL）列表。當網路爬蟲存取這些統一資源定位器時，它們會甄別出頁面上所有的超連結，並將它們寫入一張「待訪列表」，即所謂爬行疆域（英語：crawl frontier）。此疆域上的URL將會被按照一套策略迴圈來存取。如果爬蟲在執行的過程中複製歸檔和儲存網站上的資訊，這些檔案通常儲存，使他們可以較容易的被檢視。閱讀和瀏覽他們儲存的網站上並即時更新的資訊，這些被儲存的網頁又被稱為「快照」。越大容量的網頁意味著網路爬蟲只能在給予的時間內下載越少部分的網頁，所以要優先考慮其下載。高變化率意味著網頁可能已經被更新或者被取代。一些伺服器端軟體生成的URL（統一資源定位符）也使得網路爬蟲很難避免檢索到重複內容。\n",
      "\n",
      "但是網際網路的資源卷帙浩繁，這也意味著網路爬蟲只能在一定時間內下載有限數量的網頁，因此它需要衡量優先順序的下載方式。有時候網頁出現、更新和消失的速度很快，也就是說網路爬蟲下載的網頁在幾秒後就已經被修改或甚至刪除了。這些都是網路爬蟲設計師們所面臨的兩個問題。\n",
      "\n",
      "再者，伺服器端軟體所生成的統一資源位址數量龐大，以至網路爬蟲難免也會採集到重複的內容。根據超文字傳輸協定，無盡組合的參數所返回的頁面中，只有很少一部分確實傳回正確的內容。例如：數張快照陳列室的網站，可能通過幾個參數，讓用戶選擇相關快照：其一是通過四種方法對快照排序，其二是關於快照解析度的的三種選擇，其三是兩種檔案格式，另加一個用戶可否提供內容的選擇，這樣對於同樣的結果會有48種（4*3*2）不同的統一資源位址與其關聯。這種數學組合替網路爬蟲造成了麻煩，因為它們必須越過這些無關指令碼變化的組合，尋找不重複的內容。\n",
      "\n",
      "爬蟲的實現由以下策略組成：[4]\n",
      "\n",
      "爬蟲可能只想搜尋HTML頁面而避免其他MIME 類型。為了只請求HTML資源，爬蟲在抓取整個以GET方式請求的資源之前，通過建立HTTP的HEAD請求來決定網路資源的MIME類型。為了避免發出過多的請求，爬蟲會檢查URL和只請求那些以某些字元（如.html, .htm, .asp, .aspx, .php, .jsp, .jspx 或 / ）作為字尾的URL。這個策略可能會跳過很多HTML網路資源。\n",
      "\n",
      "有些爬蟲還能避免請求一些帶有「?」的資源（動態生成）。為了避免掉入從網站下載無限量的URL的爬蟲陷阱。不過假若網站重寫URL以簡化URL的目的，這個策略就變得不可靠了。\n",
      "\n",
      "爬蟲通常使用某些URL規格化的方式以避免資源的重複爬取。URL規格化，指的是以某種一致的方式修改和標準化URL的過程。這個過程有各種各樣的處理規則，包括統一轉換為小寫、移除「.」和「..」片段，以及在非空路徑里插入斜杆。\n",
      "\n",
      "有些爬蟲希望從指定的網站中儘可能地爬取資源。而路徑上移爬蟲就是為了能爬取每個URL里提示出的每個路徑。[5] 例如，給定一個Http的種子URL: http://llama.org/hamster/monkey/page.html ，要爬取 /hamster/monkey/ ， /hamster/ 和 / 。Cothey發現路徑能非常有效地爬取獨立的資源，或以某種規律無法在站內部連結接爬取到的資源。\n",
      "\n",
      "對於爬蟲來說，一個頁面的重要性也可以說是，給定查詢條件一個頁面相似效能起到的作用。網路爬蟲要下載相似的網頁被稱為主題爬蟲或局部爬蟲。這個主題爬蟲或局部爬蟲的概念第一次被Filippo Menczer[6][7] 和  Soumen Chakrabarti 等人提出的。[8]\n",
      "\n",
      "網站的屬性之一就是經常動態變化，而爬取網站的一小部分往往需要花費幾個星期或者幾個月。等到網站爬蟲完成它的爬取，很多事件也已經發生了，包括增加、更新和刪除。\n",
      "在搜尋引擎的角度，因為沒有檢測這些變化，會導致儲存了過期資源的代價。最常用的估價函式是新鮮度和過時性。\n",
      "新鮮度：這是一個衡量抓取內容是不是準確的二元值。在時間t內，倉庫中頁面p的新鮮度是這樣定義的：\n",
      "\n",
      "過時性:這是一個衡量本地已抓取的內容過時程度的指標。在時間t時，倉庫中頁面p的時效性的定義如下：\n",
      "\n",
      "爬蟲相比於人，可以有更快的檢索速度和更深的層次，所以，他們可能使一個站點癱瘓。不需要說一個單獨的爬蟲一秒鐘要執行多條請求，下載大的檔案。一個伺服器也會很難回應多執行緒爬蟲的請求。\n",
      "就像Koster所注意的那樣，爬蟲的使用對很多工作都是很有用的，但是對一般的社區，也需要付出代價。使用爬蟲的代價包括：[9]\n",
      "\n",
      "對這些問題的局部解決方法是漫遊器排除協定（Robots exclusion protocol），也被稱為robots.txt議定書[10]，這份協定是讓管理員指明網路伺服器的不應該爬取的約定。這個標準沒有包括重新存取一台伺服器的間隔的建議，雖然設定存取間隔是避免伺服器超載的最有效辦法。最近的商業搜尋引擎，如Google，Ask Jeeves，MSN和Yahoo可以在robots.txt中使用一個額外的 「Crawl-delay」參數來指明請求之間的延遲。\n",
      "\n",
      "一個並列爬蟲是並列執行多個行程的爬蟲。它的目標是最大化下載的速度，同時儘量減少並列的開銷和下載重複的頁面。為了避免下載一個頁面兩次，爬蟲系統需要策略來處理爬蟲執行時新發現的URL，因為同一個URL位址，可能被不同的爬蟲行程抓到。\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 模擬封包的標頭\n",
    "headers = {\n",
    "    'authority': 'zh.wikipedia.org',\n",
    "    'method': 'GET',\n",
    "    'path': '/wiki/' + root_keyword_link,\n",
    "    'scheme': 'https',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7,zh-CN;q=0.6',\n",
    "    'cookie': 'GeoIP=TW:TPE:Taipei:25.05:121.53:v4; TBLkisOn=0; mwPhp7Seed=8b8; WMF-Last-Access-Global=04-Jun-2019; WMF-Last-Access=04-Jun-2019',\n",
    "    'dnt': '1',\n",
    "    #'if-modified-since': 'Tue, 04 Jun 2019 12:03:22 GMT',\n",
    "    'referer': 'https://zh.wikipedia.org/wiki/Wikipedia:%E9%A6%96%E9%A1%B5',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'\n",
    "}    \n",
    "\n",
    "url = 'https://zh.wikipedia.org' + root_keyword_link  # 組合關鍵字查詢URL\n",
    "resp = requests.get(url, headers=headers)\n",
    "resp.encoding = 'utf-8'\n",
    "\n",
    "html = BeautifulSoup(resp.text, \"lxml\")\n",
    "content = html.find(name='div', attrs={'id':'mw-content-text'}).find_all(name='p')\n",
    "\n",
    "#\n",
    "# 解析回傳資料，並萃取文章內容\n",
    "#\n",
    "for paragraph in content:\n",
    "    print(paragraph.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 範例2：從爬取的文章內容中，擷取出有外部連結的關鍵字。這些關鍵字在文章中是以藍色字體顯示，會連到外部的網頁，並解釋其內容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "外部連結: [全球資訊網] /wiki/%E4%B8%87%E7%BB%B4%E7%BD%91\n",
      "外部連結: [網路機器人] /wiki/%E7%BD%91%E7%BB%9C%E6%9C%BA%E5%99%A8%E4%BA%BA\n",
      "外部連結: [網路] /wiki/%E7%BD%91%E7%BB%9C%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E\n",
      "外部連結: [搜尋引擎] /wiki/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E\n",
      "外部連結: [robots.txt] /wiki/Robots.txt\n",
      "外部連結: [網站] /wiki/%E7%BD%91%E7%AB%99\n",
      "外部連結: [超連結] /wiki/%E8%B6%85%E9%80%A3%E7%B5%90\n",
      "外部連結: [HTML] /wiki/HTML\n",
      "外部連結: [網頁] /wiki/%E7%B6%B2%E9%A0%81\n",
      "外部連結: [網際網路] /wiki/%E4%BA%92%E8%81%94%E7%BD%91\n",
      "外部連結: [伺服器] /wiki/%E6%9C%8D%E5%8A%A1%E5%99%A8\n",
      "外部連結: [超文字傳輸協定] /wiki/%E8%B6%85%E6%96%87%E6%9C%AC%E5%82%B3%E8%BC%B8%E5%8D%94%E5%AE%9A\n"
     ]
    }
   ],
   "source": [
    "for ext_link in content:\n",
    "    a_tag = ext_link.find_all('a', href=re.compile(\"^(/wiki/)((?!;)\\S)*$\"))\n",
    "    if len(a_tag) > 0:\n",
    "        for link_string in a_tag:\n",
    "            a_link = link_string[\"href\"]       # 外部連結的網址\n",
    "            a_keyword = link_string.get_text()  # 外部連結的中文名稱\n",
    "            print(\"外部連結: [%s] %s\" % (a_keyword, a_link))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業：接下來定義一個爬蟲函數，這個函數的主要工作為：\n",
    "### (1) 爬取當前關鍵字的解釋，並存入檔案(因為文章內容太多會佔滿整個頁面，所以存程檔案，方便後續檢視)\n",
    "### (2) 萃取出當前關鍵字所引用的外部連結，當作新的查詢關鍵字\n",
    "### (3) 把第(2)擷取到的關鍵字當作新的關鍵字，回到第(1)步，爬取新的關鍵字解釋。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WikiArticle(key_word_link, key_word, recursive):\n",
    "    \n",
    "    if (recursive <= max_recursive_depth):\n",
    "        print(\"遞迴層[%d] - %s (%s)\" % (recursive, key_word_link, key_word))\n",
    "        \n",
    "        # 模擬封包的標頭\n",
    "        headers = {\n",
    "            'authority': 'zh.wikipedia.org',\n",
    "            'method': 'GET',\n",
    "            'path': '/wiki/' + key_word_link,\n",
    "            'scheme': 'https',\n",
    "            'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3',\n",
    "            'accept-encoding': 'gzip, deflate, br',\n",
    "            'accept-language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7,zh-CN;q=0.6',\n",
    "            'cookie': 'GeoIP=TW:TPE:Taipei:25.05:121.53:v4; TBLkisOn=0; mwPhp7Seed=8b8; WMF-Last-Access-Global=04-Jun-2019; WMF-Last-Access=04-Jun-2019',\n",
    "            'dnt': '1',\n",
    "            #'if-modified-since': 'Tue, 04 Jun 2019 12:03:22 GMT',\n",
    "            'referer': 'https://zh.wikipedia.org/wiki/Wikipedia:%E9%A6%96%E9%A1%B5',\n",
    "            'upgrade-insecure-requests': '1',\n",
    "            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36'\n",
    "        }    \n",
    "\n",
    "        url = 'https://zh.wikipedia.org' + key_word_link  # 組合關鍵字查詢URL\n",
    "        resp = requests.get(url, headers=headers)\n",
    "        resp.encoding = 'utf-8'\n",
    "\n",
    "        html = BeautifulSoup(resp.text, \"lxml\")\n",
    "        content = html.find(name='div', attrs={'id':'mw-content-text'}).find_all(name='p')\n",
    "        \n",
    "        #\n",
    "        # Part 1: 請參考範例1，爬取當前關鍵字的文章內容。\n",
    "        #         因為內容太多，我們把它寫入檔案，並以關鍵字作為檔案名稱，以便稍後查閱內容。\n",
    "        #         請先建立一個名為\"WikiArticle\"的資料夾，爬取到的文章內容會放在這個資料夾底下。\n",
    "        #\n",
    "        import os\n",
    "        output_dir = 'WikiArticle'\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        for ext_link in content:\n",
    "            a_tag = ext_link.find_all('a', href=re.compile(\"^(/wiki/)((?!;)\\S)*$\"))\n",
    "            if len(a_tag) > 0:\n",
    "                for link_string in a_tag:\n",
    "                    a_link = link_string[\"href\"]     # 外部連結的網址\n",
    "                    a_keyword = link_string.get_text()     # 外部連結的中文名稱\n",
    "                    # 將 '/' 換成 '_'\n",
    "                    c = ''\n",
    "                    for j in [i.replace('/', '_') for i in a_keyword]:\n",
    "                        c += j\n",
    "                    a_keyword = c\n",
    "                    \n",
    "                    url_in = 'https://zh.wikipedia.org/'+a_link\n",
    "                    resp = requests.get(url_in, headers=headers)\n",
    "                    resp.encoding = 'utf-8'\n",
    "                    html = BeautifulSoup(resp.text, \"lxml\")\n",
    "                    content = html.find(name='div', attrs={'id': 'mw-content-text'}).find_all(name='p')     # <p>代表跳下一段落(中間空一行)\n",
    "                    for paragraph in content:\n",
    "                        with open(output_dir+'/'+a_keyword+'.txt', 'a', encoding='utf-8') as f:\n",
    "                            f.write(paragraph.get_text()+'\\n')\n",
    "        #\n",
    "        # Part 2: 請參考範例2，萃取出本篇文章中所延伸引用的外部連結，並儲存在external_link_dict\n",
    "        #\n",
    "        external_link_dict = dict({})\n",
    "        for ext_link in content:\n",
    "            a_tag = ext_link.find_all('a', href=re.compile(\"^(/wiki/)((?!;)\\S)*$\"))\n",
    "            if len(a_tag) > 0:\n",
    "                for link_string in a_tag:\n",
    "                    a_link = link_string[\"href\"]     # 外部連結的網址\n",
    "                    a_keyword = link_string.get_text()     # 外部連結的中文名稱\n",
    "                    external_link_dict[a_keyword] = a_link\n",
    "        #\n",
    "        # Part 3: 將Part 2所收集的外部連結，當作新的關鍵字，繼續迭代深入爬蟲\n",
    "        #\n",
    "        if (len(external_link_dict) > 0):\n",
    "            recursive += 1     # 遞迴深度加1\n",
    "            \n",
    "            for k, v in external_link_dict.items():\n",
    "                WikiArticle(v, k, recursive)  # 再次呼叫同樣的函數，執行同樣的流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 執行前個步驟定義好的爬蟲主程式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "遞迴層[0] - /wiki/%E7%B6%B2%E8%B7%AF%E7%88%AC%E8%9F%B2 (網路爬蟲)\n",
      "遞迴層[1] - /wiki/%E8%B6%85%E5%AA%92%E9%AB%94 (超媒體)\n",
      "遞迴層[1] - /wiki/%E5%BA%94%E7%94%A8%E5%B1%82 (應用層)\n",
      "遞迴層[1] - /wiki/%E7%BD%91%E7%BB%9C%E4%BC%A0%E8%BE%93%E5%8D%8F%E8%AE%AE (協定)\n",
      "遞迴層[1] - /wiki/%E5%85%A8%E7%90%83%E8%B3%87%E8%A8%8A%E7%B6%B2 (全球資訊網)\n",
      "遞迴層[1] - /wiki/HTML (HTML)\n",
      "遞迴層[1] - /wiki/HTTPS (HTTPS)\n",
      "遞迴層[1] - /wiki/%E7%BB%9F%E4%B8%80%E8%B5%84%E6%BA%90%E6%A0%87%E5%BF%97%E7%AC%A6 (統一資源識別碼)\n",
      "遞迴層[1] - /wiki/%E6%8F%90%E5%A7%86%C2%B7%E6%9F%8F%E5%85%A7%E8%8C%B2-%E6%9D%8E (提姆·柏內茲-李)\n",
      "遞迴層[1] - /wiki/%E6%AD%90%E6%B4%B2%E6%A0%B8%E5%AD%90%E7%A0%94%E7%A9%B6%E7%B5%84%E7%B9%94 (歐洲核子研究組織)\n",
      "遞迴層[1] - /wiki/%E4%B8%87%E7%BB%B4%E7%BD%91%E8%81%94%E7%9B%9F (全球資訊網協會)\n",
      "遞迴層[1] - /wiki/%E4%BA%92%E8%81%94%E7%BD%91%E5%B7%A5%E7%A8%8B%E4%BB%BB%E5%8A%A1%E7%BB%84 (網際網路工程任務組)\n",
      "遞迴層[1] - /wiki/RFC (RFC)\n",
      "遞迴層[1] - /wiki/HTTP/2 (HTTP/2)\n",
      "遞迴層[1] - /wiki/%E4%BC%A0%E8%BE%93%E6%8E%A7%E5%88%B6%E5%8D%8F%E8%AE%AE (TCP協定)\n",
      "遞迴層[1] - /wiki/%E7%B6%B2%E9%A0%81%E7%80%8F%E8%A6%BD%E5%99%A8 (網頁瀏覽器)\n",
      "遞迴層[1] - /wiki/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB (網路爬蟲)\n",
      "遞迴層[1] - /wiki/%E9%80%9A%E8%A8%8A%E5%9F%A0 (埠)\n",
      "遞迴層[1] - /wiki/%E4%BB%A3%E7%90%86%E6%9C%8D%E5%8A%A1%E5%99%A8 (代理伺服器)\n",
      "遞迴層[1] - /wiki/%E7%BD%91%E5%85%B3 (閘道器)\n",
      "遞迴層[1] - /wiki/%E9%9A%A7%E9%81%93 (隧道)\n",
      "遞迴層[1] - /wiki/TCP/IP (TCP/IP)\n",
      "遞迴層[1] - /wiki/HTTP%E7%8A%B6%E6%80%81%E7%A0%81#405 (狀態碼405)\n",
      "遞迴層[1] - /wiki/HTTP%E7%8A%B6%E6%80%81%E7%A0%81#501 (狀態碼501)\n",
      "遞迴層[1] - /wiki/%E8%B6%85%E9%93%BE%E6%8E%A5 (超連結)\n",
      "遞迴層[1] - /wiki/%E5%86%AA%E7%AD%89 (冪等(idempotence))\n",
      "遞迴層[1] - /wiki/%E5%90%91%E4%B8%8B%E5%85%BC%E5%AE%B9 (向下相容)\n",
      "遞迴層[1] - /wiki/Wikipedia:%E5%88%97%E6%98%8E%E6%9D%A5%E6%BA%90 ([來源請求])\n",
      "遞迴層[1] - /wiki/HTTP%E7%AE%A1%E7%BA%BF%E5%8C%96 (管道方式)\n",
      "遞迴層[1] - /wiki/HTTP%E7%8A%B6%E6%80%81%E7%A0%81 (狀態代碼)\n",
      "遞迴層[1] - /wiki/HTTP_404 (404 Not Found)\n",
      "遞迴層[1] - /wiki/%E4%BC%A0%E8%BE%93%E6%8E%A7%E5%88%B6%E5%8D%8F%E8%AE%AE (TCP連線)\n",
      "遞迴層[1] - /wiki/%E5%BB%B6%E8%BF%9F_(%E5%B7%A5%E7%A8%8B%E5%AD%A6) (等待時間)\n",
      "遞迴層[1] - /wiki/%E6%8F%A1%E6%89%8B_(%E6%8A%80%E6%9C%AF) (TCP交握程式)\n",
      "遞迴層[1] - /wiki/%E5%88%86%E5%9D%97%E4%BC%A0%E8%BE%93%E7%BC%96%E7%A0%81 (分塊傳輸編碼)\n",
      "遞迴層[1] - /wiki/%E7%B7%A9%E8%A1%9D%E5%99%A8 (緩衝器)\n",
      "遞迴層[1] - /wiki/%E4%BD%BF%E7%94%A8%E8%80%85%E7%B6%93%E9%A9%97 (用戶感受到的)\n",
      "遞迴層[1] - /wiki/Gopher_(%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE) (Gopher)\n",
      "遞迴層[1] - /wiki/SPDY (SPDY)\n",
      "遞迴層[1] - /wiki/Google (Google)\n"
     ]
    }
   ],
   "source": [
    "# 定義爬取的遞迴深度。深度不要訂太深，否則會爬很久。\n",
    "max_recursive_depth = 1\n",
    "\n",
    "WikiArticle(root_keyword_link, input_keyword, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
